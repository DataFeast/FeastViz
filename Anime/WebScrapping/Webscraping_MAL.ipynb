{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "environmental-contemporary",
   "metadata": {},
   "source": [
    "# Scrapping Animes info using Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "focused-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request,sys,time\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-orbit",
   "metadata": {},
   "source": [
    "## Making Simple requests\n",
    "\n",
    "We would use \"Horror\" genre from [My Anime List](https://myanimelist.net/anime/genre/14/Horror)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "republican-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://myanimelist.net/anime/genre/14/Horror\"\n",
    "\n",
    "page = requests.get(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-macro",
   "metadata": {},
   "source": [
    "## Extracting content from HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "guilty-forth",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-dance",
   "metadata": {},
   "source": [
    "Here we need to find all the links that connect with the animes pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "advisory-preservation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "links = soup.find_all(\"a\", attrs = {'class' : 'link-title'})\n",
    "\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "saved-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_links = []\n",
    "for link in links:\n",
    "    anime_links.append(link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cutting-nickel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://myanimelist.net/anime/22319/Tokyo_Ghoul',\n",
       " 'https://myanimelist.net/anime/22535/Kiseijuu__Sei_no_Kakuritsu',\n",
       " 'https://myanimelist.net/anime/27899/Tokyo_Ghoul_√A',\n",
       " 'https://myanimelist.net/anime/37779/Yakusoku_no_Neverland',\n",
       " 'https://myanimelist.net/anime/11111/Another',\n",
       " 'https://myanimelist.net/anime/226/Elfen_Lied',\n",
       " 'https://myanimelist.net/anime/8074/Highschool_of_the_Dead',\n",
       " 'https://myanimelist.net/anime/6880/Deadman_Wonderland',\n",
       " 'https://myanimelist.net/anime/36511/Tokyo_Ghoul_re',\n",
       " 'https://myanimelist.net/anime/35120/Devilman__Crybaby']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anime_links[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-pierce",
   "metadata": {},
   "source": [
    "Super!!!!\n",
    "\n",
    "### Explore others page\n",
    "We extract all the links from the first page but now we need to search how to explore the others page without knowing how many pages are present. Maybe we can use a `while` loop.\n",
    "\n",
    "For example, **Horror** genre only have 5 pages, so at the moment to reach the sixth page we need to break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "electrical-regulation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Page https://myanimelist.net/anime/genre/14/Horror?page=1 and status code 200\n",
      "Correct Page https://myanimelist.net/anime/genre/14/Horror?page=2 and status code 200\n",
      "Correct Page https://myanimelist.net/anime/genre/14/Horror?page=3 and status code 200\n",
      "Correct Page https://myanimelist.net/anime/genre/14/Horror?page=4 and status code 200\n",
      "Correct Page https://myanimelist.net/anime/genre/14/Horror?page=5 and status code 200\n",
      "Correct Page https://myanimelist.net/anime/genre/14/Horror?page=6 and status code 404\n"
     ]
    }
   ],
   "source": [
    "num_page = 1 \n",
    "status = 202\n",
    "while status != 404:\n",
    "    URL = \"https://myanimelist.net/anime/genre/14/Horror?page=\" + str(num_page)\n",
    "    page = requests.get(URL)\n",
    "    status = page.status_code\n",
    "    num_page += 1\n",
    "    print(f\"Correct Page {URL} and status code {page.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-sally",
   "metadata": {},
   "source": [
    "nice!!\n",
    "\n",
    "With this we can explore all the page from the genre that we wanted.\n",
    "\n",
    "## Explore each anime\n",
    "\n",
    "Now that we have the link we can explore each anime and extract other information. We would use [Tokyo_Ghoul](https://myanimelist.net/anime/22319/Tokyo_Ghoul) as example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "satisfactory-simple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "URL = \"https://myanimelist.net/anime/22319/Tokyo_Ghoul\"\n",
    "\n",
    "page = requests.get(URL)\n",
    "print(page.status_code)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-fighter",
   "metadata": {},
   "source": [
    "Looking the web design the easier way is find the all \\<tr\\> and the the first \\<td\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "polyphonic-mailman",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_card = soup.find(\"tr\")\n",
    "anime_info = anime_card.find(\"td\")\n",
    "\n",
    "anime_english = anime_info.find(\"span\", text = \"English:\").next_sibling\n",
    "anime_japanese = anime_info.find(\"span\", text = \"Japanese:\").next_sibling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "responsible-republican",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_episodes = anime_info.find(\"span\", text = \"Episodes:\").next_sibling\n",
    "anime_source = anime_info.find(\"span\", text = \"Source:\").next_sibling\n",
    "anime_ratings = anime_info.find(\"span\", text = \"Rating:\").next_sibling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "annoying-fabric",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokyo Ghoul\n",
      "東京喰種-トーキョーグール-\n",
      "12\n",
      "Manga\n",
      "R - 17+ (violence & profanity)\n"
     ]
    }
   ],
   "source": [
    "print(anime_english.strip())\n",
    "print(anime_japanese.strip())\n",
    "print(anime_episodes.strip())\n",
    "print(anime_source.strip())\n",
    "print(anime_ratings.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "previous-recycling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Action'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anime_info.find_all(\"span\", attrs={'itemprop': 'genre'})[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "assumed-greene",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Action;Action'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ['Action', 'Action']\n",
    "';'.join(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-photographer",
   "metadata": {},
   "source": [
    "Here the important aspect is that we would need to find the sections from we are interested. Remember that if there is a error or the sections was not found we would need to make an exception.\n",
    "\n",
    "To extract other information is a little harder becuase they don't have a specific ID or class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cordless-credit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Studio Pierrot'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anime_details = soup.find(\"div\", attrs={'class':'anime-detail-header-stats'})\n",
    "score = anime_details.find(\"div\", attrs={'class': 'score-label'}).text\n",
    "scored_by = anime_details.find(\"div\", attrs={'class': 'score'})['data-user'].replace(\"users\", \"\").replace(\",\", \"\")\n",
    "ranked = anime_details.find(\"span\", attrs = {'class': 'numbers ranked'}).text\n",
    "popularity = anime_details.find(\"span\", attrs = {'class': 'numbers popularity'}).text\n",
    "members = anime_details.find(\"span\", \n",
    "                             attrs = {'class': 'numbers members'}).text.replace(\"Members \", \"\").replace(\",\", \"\")\n",
    "season = anime_details.find(\"span\", attrs = {'class': 'information season'}).text\n",
    "anime_type = anime_details.find(\"span\", attrs = {'class': 'information type'}).text\n",
    "studio = anime_details.find(\"span\", attrs = {'class': 'information studio author'}).text\n",
    "\n",
    "studio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-merit",
   "metadata": {},
   "source": [
    "Finally we need to make a datafrae with all animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-novelty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "continuing-explosion",
   "metadata": {},
   "source": [
    "## Second version web scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "blocked-storage",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://myanimelist.net/anime/genre/14/Horror\"\n",
    "\n",
    "page = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "later-chess",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "animes = soup.find_all(\"div\", attrs={'class':'seasonal-anime'})\n",
    "print(len(animes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-routine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-housing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-commercial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-dominant",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataFeast",
   "language": "python",
   "name": "datafeast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
